---
layout: page
title: Code Interview
nav_exclude: true
---


# Code Narration & Interview Review Platform

## Project Summary

The Code Narration & Interview Review Platform is designed to support students in developing and demonstrating conceptual understanding of code through explanation and narration, rather than code production alone. The platform uses AI-assisted feedback to help students practice technical interviews by explaining existing code, articulating design decisions, and reflecting on their own reasoning.

This project aligns with the BOBPE mission by leveraging bot-based systems to approximate one-on-one coaching and feedback in contexts where individualized instruction is traditionally difficult to scale.

---

## Educational Problem Addressed

In both coursework and technical interviews, students are often evaluated on their ability to reason about code rather than simply write it. However, most instructional settings emphasize code production, leaving students underprepared to explain, critique, and reason about existing implementations.

Providing individualized feedback on spoken or written code explanations is time-intensive and difficult to scale, particularly in large courses or interview preparation settings. As a result, students receive limited formative feedback on an essential skill.

---

## How AI Is Used

AI is used to:
- Analyze student explanations and narrations of provided code
- Identify gaps in conceptual understanding, reasoning, or clarity
- Provide structured, personalized feedback on explanations
- Prompt deeper reflection through follow-up questions
- Emphasize understanding, trade-offs, and intent rather than correctness alone

The AI does not generate interview answers for students. Instead, it evaluates and responds to student-provided explanations.

---

## Student Experience

Students interact with the platform by:
1. Uploading code or selecting from pre-populated code examples
2. Providing a narrated or written explanation of the codeâ€™s behavior and design
3. Receiving AI-generated feedback on clarity, correctness, and depth of understanding
4. Responding to follow-up prompts that encourage refinement and reflection

This workflow mirrors authentic technical interviews and code review scenarios.

---

## Instructor and Reviewer Experience

Instructors and mentors can use the platform to:
- Provide scalable, consistent feedback on code understanding
- Focus assessment on reasoning and explanation
- Identify common misunderstandings across students
- Reduce time spent on repetitive, low-level feedback

The system supports a human-in-the-loop model, allowing reviewers to inspect and edit AI-generated evaluations as needed.

---

## Deployment Status

**Status:** Research prototype / Pilot use  
The platform has been developed and demonstrated in controlled settings and is being refined based on pilot usage and feedback.

---

## Artifacts and Links

- [Demo and workflow videos](https://drive.google.com/drive/folders/1eR3KMVluemarJIA2y9WVUBnLiq5IVU1p)

- [Research paper](https://drive.google.com/file/d/1bvk0q9eY0lwCQ5CBN-TpqIliGhlTfR_E/view)

---

## Collaboration and Credit

This project was developed in collaboration with **Nathan**, whose contributions were central to the platform design, interaction model, and feedback architecture.

---

## Alignment with BOBPE Mission

This project exemplifies the BOBPE focus on understanding over production by using AI to scale individualized feedback on reasoning, explanation, and metacognition. It demonstrates how bot-based systems can extend high-impact instructional practices into authentic, workforce-relevant learning contexts without increasing instructor workload.
