# LaTeX AI Grader Platform

This repository contains an AI-assisted grading system for LaTeX-based STEM assignments.  
It supports automated evaluation with multiple AI models, human review workflows, and Canvas integration.

---

## ğŸ” Overview

The platform is designed around a two-database architecture:

### **1. `primary.db` (global)**
Stores:
- Canvas student IDs  
- Submission metadata  
- Sanitized LaTeX  
- PDF render paths  
- Human review decisions  
- Assignment linkage  

### **2. `evaluations.db` (global)**
Located at:

data/ai_outputs/evaluations.db


Contains:
- Multiple model evaluations per student (scoped by course_id + assignment_id)  
- Raw + structured AI outputs  
- Metadata (model version, parameters, timestamps)  
- Human reviews (linked to evaluations)

This separation makes the system scalable, modular, and research-friendly.

---

## ğŸ—‚ï¸ Directory Structure

```

latex_grader/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â”œâ”€â”€ db_config.yaml
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ expert/
â”‚       â”œâ”€â”€ personality/
â”‚       â””â”€â”€ questions/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ incoming/
â”‚   â”œâ”€â”€ sanitized/
â”‚   â”œâ”€â”€ compiled_pdfs/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ ai_outputs/  # per-assignment evaluation DBs
â”‚
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”œâ”€â”€ evaluator_openai.py
â”‚   â”œâ”€â”€ evaluator_ollama.py
â”‚   â”œâ”€â”€ dual_evaluator.py
â”‚   â””â”€â”€ overseer_evaluator.py   # future expansion
â”‚
â”œâ”€â”€ preprocessor/
â”‚   â”œâ”€â”€ strip_preamble.py
â”‚   â”œâ”€â”€ sanitize_tex.py
â”‚   â”œâ”€â”€ compile_pdf.py
â”‚   â””â”€â”€ utilities.py
â”‚
â”œâ”€â”€ reviewer_ui/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ static/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_all.py
â”‚   â”œâ”€â”€ seed_prompts.py
â”‚   â”œâ”€â”€ export_results.py
â”‚   â””â”€â”€ upload_canvas.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

```

---

## âš™ï¸ Workflow

1. **Submission Ingestion**  
   Student .tex â†’ PII removed â†’ sanitized â†’ PDF compiled  

2. **AI Evaluation**  
   - Multiple evaluator models run (OpenAI, Ollama, etc.)  
   - Results stored in the assignment-specific evaluation DB  

3. **Human Review UI**  
   - Left: rendered student PDF  
   - Right: editable AI-generated feedback  
   - Save corrected results into `evaluations.db`

4. **Export & LMS Integration**  
   Scripts can export summaries or upload results to Canvas.

---

## ğŸ›  Setup

python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

Make sure to have tectonic installed on the system (for macos)
```

brew install tectonic

```

Install dependencies:
```

pip install -r requirements.txt

```

For testing:
#make some sample student .tex files (data/incoming/raw_submissions_XXX_YYY) and expert files (config/prompts/expert)
use chatgpt to make these files..replace with actual student files when they come in..
#then run the sanitize_and_ingest.py:
python ./preprocessor/sanitize_and_ingest.py --course 123456 --assignment 987654 

preprocessor/santizie_and_ingest.py will create data/primary.db 

#if the question jsons aren't created but instead there's just a question .tex and an expert .tex then create the json
python scripts/build_question_json.py --course 123456 --assignment 987654

#test the prompt builder by adjusting the sample files and the assignment
python -m scripts.test_prompt_builder

#copy that into your favorite chatbot and see what it says 

#test the openai call using a sample submission 
python -m scripts.test_openai_eval 

#run a batch process of everything with run_all.py (see run_all.py for more information on the tags)
python ./scripts/run_all.py \
        --course 123456 \
        --assignment 987654 \
        --model gpt-4.1-mini --progress off --run_some 2

# Moving on to the reviewer stage
python -m reviewer_ui.app

#then go to: 
http://localhost:5001/review/123456/987654/456780

# Next steps

1. add in the ability to review with but ollama, and gemini would be good as well. also a summarizer would be nice to have too, not sure how to do that - this might be overseer_evaluator. So probably just need to build a script that compiles all of that, judges the models, and provides and update overseer evaluation.

2. also it would be nice to consider some filter mechanism for multiple graders on the reviewer_ui.app. This could be as easy as a list of graders and how many submissions they'll grade. 
Also a status bar of how many submissions are left to grade.

# Longer term to make this more intergrated with Canvas. 
Create a full pipeline, where submissions are pulled from canvas into raw_submissions_course_assignment. then run the sanitize and ingestion, build question and prompt, then run_all.py, then the reviewer, then the graders do their thing, then push the results back to canvas. 

## Notes:
 1. Assume FIELD_ORDER and FIELD_TYPES in review.js are the same as given in /config/prompts/personality/base.json in the "output_format_xml"
2. To compile the questions to a pdf from just a basebones .tex file:
chmod +x compile_fragments.sh
./compile_fragments.sh ./config/prompts/expert
./compile_fragments.sh ./config/prompts/questions


---

## ğŸ“Œ License

See the included `LICENSE` file.

---

This project is designed to be modular, scalable, and suitable for large courses or research experiments involving multiple AI models.